{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Lab 1: Introduction to Big Data Processing\n",
        "\n"
      ],
      "metadata": {
        "id": "MMVC0DDXaWxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Introduction\n",
        "Big data processing involves techniques to efficiently handle and analyze datasets too large to fit into memory. This lab focuses on understanding partitioning, aggregation, sorting, and distributed systems concepts foundational to tools like MapReduce, Hadoop, and Spark."
      ],
      "metadata": {
        "id": "Xbx6Z8evm5Ma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Helper: Timed Decorator\n",
        "To evaluate the execution time of each function systematically, we can create a reusable timed decorator.\n",
        "The decorator logs the execution time of any function it wraps."
      ],
      "metadata": {
        "id": "D-r52JmlbM_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from functools import wraps\n",
        "\n",
        "function_perf_tracker = {}\n",
        "\n",
        "def timed(func):\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start_time = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "        print(f\"Function '{func.__name__}' executed in {end_time - start_time:.4f} seconds.\")\n",
        "        function_perf_tracker[func.__name__] = end_time - start_time\n",
        "        return result\n",
        "    return wrapper\n"
      ],
      "metadata": {
        "id": "qRoXGpNjbNY6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 1: Searching via Partitioning\n",
        "Efficiently search for integers in a large dataset using both naive (linear) and optimized (partitioned) approaches.\n",
        "\n"
      ],
      "metadata": {
        "id": "lnj0aGf4ao7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1: Dataset Generation\n",
        "\n",
        "Use Python's random module to generate the dataset:"
      ],
      "metadata": {
        "id": "2KahqWRkaykn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "g3vmOrIQaGUC"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Generate dataset\n",
        "with open(\"search_data.txt\", \"w\") as file:\n",
        "    for _ in range(900000):\n",
        "        file.write(f\"{random.randint(1, 1000000)}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2: Linear Search\n",
        "\n",
        "A naive approach to scan the dataset sequentially:"
      ],
      "metadata": {
        "id": "NvxJDYUSbAw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@timed\n",
        "def linear_search(filename, targets):\n",
        "    # Convert targets to a set for efficient membership checks\n",
        "    target_set = set(targets)\n",
        "    results = {target: \"NO\" for target in targets}  # Initialize results as \"NO\"\n",
        "\n",
        "    with open(filename, \"r\") as file:\n",
        "        for line in file:\n",
        "            num = int(line.strip())\n",
        "            if num in target_set:\n",
        "                results[num] = \"YES\"\n",
        "\n",
        "    # Return the results in the same order as the input targets\n",
        "    return [results[target] for target in targets]\n",
        "\n",
        "# Example usage\n",
        "targets = [5, 1000, 250000, 750000, 999999]\n",
        "print(linear_search(\"search_data.txt\", targets))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJhCKiwKbC41",
        "outputId": "4ce112c0-cde3-44cf-90fd-7a231e614603"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'linear_search' executed in 0.2944 seconds.\n",
            "['NO', 'YES', 'NO', 'YES', 'YES']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Partitioned Search\n",
        "\n",
        "Partition the dataset into 1,000 smaller files:"
      ],
      "metadata": {
        "id": "E8ICRpona8Xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def partition_dataset(input_file, partitions=1000):\n",
        "    partition_files = [open(f\"partition_{i}.txt\", \"w\") for i in range(partitions)]\n",
        "\n",
        "    with open(input_file, \"r\") as file:\n",
        "        for line in file:\n",
        "            num = int(line.strip())\n",
        "            partition_files[num % partitions].write(line)\n",
        "\n",
        "    for pf in partition_files:\n",
        "        pf.close()\n",
        "\n",
        "partition_dataset(\"search_data.txt\")"
      ],
      "metadata": {
        "id": "Kez4e36Eb7VK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Search in the relevant partition file:**"
      ],
      "metadata": {
        "id": "ExnrNov0cC_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@timed\n",
        "def partitioned_search(partitions, targets):\n",
        "    target_set = set(targets)\n",
        "    results = {target: \"NO\" for target in targets}  # Initialize results as \"NO\"\n",
        "    keys = set([n%partitions for n in targets])\n",
        "    for key in keys:\n",
        "        partition_file = f\"partition_{key}.txt\"\n",
        "        with open(partition_file, \"r\") as file:\n",
        "            for line in file:\n",
        "              num = int(line.strip())\n",
        "              if num in target_set:\n",
        "                  results[num] = \"YES\"\n",
        "                  target_set.remove(num)\n",
        "\n",
        "    return [results[target] for target in targets]\n",
        "\n",
        "print(partitioned_search(1000, targets))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hvw1XOdncFky",
        "outputId": "56669eac-a749-4578-d98a-410d597288b1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'partitioned_search' executed in 0.0017 seconds.\n",
            "['NO', 'YES', 'NO', 'YES', 'YES']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Key Takeaway\n",
        "Partitioning significantly reduces the search space, mimicking distributed file systems' efficiency."
      ],
      "metadata": {
        "id": "7mXxVSxCck1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 2: Grouping and Aggregation\n",
        "\n",
        "Aggregate data by grouping keys, using naive and partitioned methods."
      ],
      "metadata": {
        "id": "-HMKVGU3csJg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1: Dataset Generation\n"
      ],
      "metadata": {
        "id": "IrLN9Jd0cz3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate dataset\n",
        "with open(\"group_data.txt\", \"w\") as file:\n",
        "    for _ in range(30000000):\n",
        "        k = random.randint(1, 7)\n",
        "        v = random.randint(1, 1000)\n",
        "        file.write(f\"{k},{v}\\n\")"
      ],
      "metadata": {
        "id": "K_KsSQUscP76"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2: Naive Grouping\n",
        "\n",
        "Read the file and compute aggregation:"
      ],
      "metadata": {
        "id": "fS6LQ4KmdPIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "@timed\n",
        "def naive_grouping(filename):\n",
        "    aggregation = defaultdict(int)\n",
        "    with open(filename, \"r\") as file:\n",
        "        for line in file:\n",
        "            k, v = map(int, line.strip().split(\",\"))\n",
        "            aggregation[k] += v\n",
        "    return sorted(aggregation.items())\n",
        "\n",
        "print(naive_grouping(\"group_data.txt\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_0CIA2CdWDd",
        "outputId": "9bb65dc1-4800-4a29-bfcc-003cb4a1291e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'naive_grouping' executed in 27.9796 seconds.\n",
            "[(1, 2148384856), (2, 2144520357), (3, 2140954403), (4, 2143625377), (5, 2145918822), (6, 2147204075), (7, 2145779744)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 3: Partitioned Grouping\n",
        "\n",
        "**Partition the dataset:**\n"
      ],
      "metadata": {
        "id": "L1rbOI8qdaCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def partition_group_data(input_file, partitions=10):\n",
        "    partition_files = [open(f\"group_partition_{i}.txt\", \"w\") for i in range(partitions)]\n",
        "\n",
        "    with open(input_file, \"r\") as file:\n",
        "        for line in file:\n",
        "            k, v = map(int, line.strip().split(\",\"))\n",
        "            partition_files[v % partitions].write(line)\n",
        "\n",
        "    for pf in partition_files:\n",
        "        pf.close()\n",
        "\n",
        "partition_group_data(\"group_data.txt\", 100)"
      ],
      "metadata": {
        "id": "NWwY7C1slrCt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aggregate**"
      ],
      "metadata": {
        "id": "XjlHi2QOltHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "@timed\n",
        "def partitioned_grouping(partitions=10):\n",
        "    final_aggregation = defaultdict(int)\n",
        "    all_local_aggregations = []\n",
        "\n",
        "    # Compute local aggregations\n",
        "    for i in range(partitions):\n",
        "        local_aggregation = defaultdict(int)\n",
        "        with open(f\"group_partition_{i}.txt\", \"r\") as file:\n",
        "            for line in file:\n",
        "                k, v = map(int, line.strip().split(\",\"))\n",
        "                local_aggregation[k] += v\n",
        "\n",
        "        all_local_aggregations.append(local_aggregation)\n",
        "\n",
        "    # Merge local aggregations\n",
        "    for local_aggregation in all_local_aggregations:\n",
        "      for k, s in local_aggregation.items():\n",
        "          final_aggregation[k] += s\n",
        "\n",
        "    return final_aggregation\n",
        "\n",
        "aggregation_sum = partitioned_grouping(100)\n",
        "print(sorted(aggregation_sum.items()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p74ie9EGdcpu",
        "outputId": "dfe18cbb-1354-4e26-de0c-3d1d33180364"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'partitioned_grouping' executed in 28.7355 seconds.\n",
            "[(1, 2146449325), (2, 2144686253), (3, 2143693767), (4, 2143779031), (5, 2144550492), (6, 2145944290), (7, 2144219893)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Key Takeaway\n",
        "Partitioning simulates distributed \"reduce\" operations, optimizing performance for large-scale data."
      ],
      "metadata": {
        "id": "yBiF1CTndrjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2bis: Sorting and Grouping by Key\n",
        "\n",
        "Perform grouping and aggregation on a sorted dataset by key, simulating how sorting helps optimize grouping operations in big data systems."
      ],
      "metadata": {
        "id": "wNTZpE8KsX_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1: Dataset Generation\n",
        "Generate the sorted dataset"
      ],
      "metadata": {
        "id": "sNVScnWNtwKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def generate_sorted_group_data(file_path, size=3000000):\n",
        "    \"\"\"\n",
        "    Generate a dataset of (k, v) pairs and save it sorted by k.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): File to save the dataset.\n",
        "        size (int): Number of (k, v) pairs to generate.\n",
        "    \"\"\"\n",
        "    dataset = [(random.randint(1, 7), random.randint(1, 1000)) for _ in range(size)]\n",
        "    dataset.sort(key=lambda x: x[0])  # Sort by k\n",
        "    with open(file_path, \"w\") as file:\n",
        "        file.writelines(f\"{k},{v}\\n\" for k, v in dataset)\n",
        "    print(f\"Generated sorted dataset and saved to {file_path}.\")\n",
        "\n",
        "generate_sorted_group_data(\"sorted_group_data.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8rfVxDItrtJ",
        "outputId": "ab5291c2-375c-4381-f181-3ae67cb9e6fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sorted dataset and saved to sorted_group_data.txt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step2:  Iterator-based Grouping\n",
        "\n",
        "Group and sum values by key using an iterator over a sorted dataset.\n"
      ],
      "metadata": {
        "id": "xwteMqhQuE7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import groupby\n",
        "\n",
        "@timed\n",
        "def iterator_based_grouping(file_path):\n",
        "    \"\"\"\n",
        "    Group and sum values by key using an iterator over a sorted dataset.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the sorted dataset.\n",
        "\n",
        "    Returns:\n",
        "        dict: Aggregated sum of values for each key.\n",
        "    \"\"\"\n",
        "    aggregation = {}\n",
        "    with open(file_path, \"r\") as file:\n",
        "        # Group consecutive lines with the same key using groupby\n",
        "        for k, group in groupby(file, key=lambda line: int(line.split(\",\")[0])):\n",
        "            aggregation[k] = sum(int(line.split(\",\")[1]) for line in group)\n",
        "    return aggregation\n",
        "\n",
        "sorted_aggregation_data = iterator_based_grouping(\"sorted_group_data.txt\")\n",
        "sorted_aggregation_data = sorted(sorted_aggregation_data.items())\n",
        "print(sorted_aggregation_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmUlGjr5uxOI",
        "outputId": "2334ea82-b50c-41da-82dc-4f8307acdfde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'iterator_based_grouping' executed in 3.0983 seconds.\n",
            "[(1, 450146929), (2, 300350528), (3, 300587224), (4, 150624761), (5, 149503408), (6, 74968250), (7, 75078409)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Grouping via Iteration\n",
        "Implement a function to iterate through the sorted dataset, grouping values by \\( k \\) as they appear."
      ],
      "metadata": {
        "id": "iQItUj3iv0Bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@timed\n",
        "def grouping_by_iteration(file_path):\n",
        "    aggregation = {}\n",
        "    current_key = None\n",
        "    current_sum = 0\n",
        "    with open(file_path, \"r\") as file:\n",
        "        for line in file:\n",
        "            k, v = map(int, line.strip().split(\",\"))\n",
        "            if k != current_key:\n",
        "                if current_key is not None:\n",
        "                    aggregation[current_key] = current_sum\n",
        "                current_key, current_sum = k, v\n",
        "            else:\n",
        "                current_sum += v\n",
        "    return aggregation\n",
        "\n",
        "aggregation_data = grouping_by_iteration(\"sorted_group_data.txt\")\n",
        "aggregation_data = sorted(aggregation_data.items())\n",
        "print(aggregation_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLiZsu0awPxc",
        "outputId": "946933e2-978e-4d8c-a221-fb49b74139ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'grouping_by_iteration' executed in 2.3080 seconds.\n",
            "[(1, 450146929), (2, 300350528), (3, 300587224), (4, 150624761), (5, 149503408), (6, 74968250)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discussion\n",
        "- Measure execution time for:\n",
        "  1. **Naive Grouping**: Reads and groups an unsorted file by scanning and aggregating in memory.\n",
        "  2. **Iterator-based Grouping**: Processes the sorted file line by line using the grouping-by-iteration method.\n",
        "- Compare the performance of the iterator-based grouping on the sorted file against the naive grouping on an unsorted dataset."
      ],
      "metadata": {
        "id": "JWFqkuU7vSYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 3: n-way Merge-Sort\n"
      ],
      "metadata": {
        "id": "ThtdBChrgC0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1: Dataset Preparation\n",
        "Generate and save the sorted lists as in the original exercise.\n"
      ],
      "metadata": {
        "id": "O4WL8F2qgNRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Generate sorted lists and save to files\n",
        "lists = [\n",
        "    sorted(random.randint(1, 100) for _ in range(10)),\n",
        "    sorted(random.randint(50, 150) for _ in range(10)),\n",
        "    sorted(random.randint(100, 200) for _ in range(10))\n",
        "]\n",
        "\n",
        "for i, lst in enumerate(lists):\n",
        "    with open(f\"list_{i}.txt\", \"w\") as file:\n",
        "        file.writelines(f\"{x}\\n\" for x in lst)\n"
      ],
      "metadata": {
        "id": "UE87kG2igTis"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2: N-way Merge-Sort Implementation\n",
        "The idea is to read numbers from all files sequentially, maintaining a pointer (or current position) in each file to track which number should be considered next."
      ],
      "metadata": {
        "id": "BotzNulggbsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@timed\n",
        "def n_way_merge(files):\n",
        "    # Open all files\n",
        "    open_files = [open(file, \"r\") for file in files]\n",
        "\n",
        "    # Read the first line from each file to initialize the pointers\n",
        "    pointers = [int(file.readline().strip()) if not file.closed else None for file in open_files]\n",
        "\n",
        "    output_filename = \"sorted_output.txt\"\n",
        "\n",
        "    # Merge the files\n",
        "    with open(output_filename, \"w\") as output_file:\n",
        "      while any(pointers):  # Continue while at least one file still has data\n",
        "          # Find the smallest value among the current pointers\n",
        "          min_value = min(value for value in pointers if value is not None)\n",
        "          output_file.write(f\"{min_value}\\n\")\n",
        "\n",
        "          # Update the pointer for the file from which the min_value came\n",
        "          min_value_index = pointers.index(min_value)\n",
        "          next_line = open_files[min_value_index].readline().strip()\n",
        "          pointers[min_value_index] = int(next_line) if next_line else None\n",
        "\n",
        "    # Close all files\n",
        "    for file in open_files:\n",
        "        file.close()\n",
        "\n",
        "    return output_filename\n",
        "\n",
        "# Example usage\n",
        "files = [f\"list_{i}.txt\" for i in range(3)]\n",
        "\n",
        "merged_output_file = n_way_merge(files)\n",
        "with open(merged_output_file, \"r\") as file:\n",
        "    merged_output = map(int, file.read().strip().split(\"\\n\"))\n",
        "    merged_output = list(merged_output)\n",
        "\n",
        "print(merged_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4L-wj2rgeQQ",
        "outputId": "7e342fd5-435b-4556-d244-6137e45046d6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'n_way_merge' executed in 0.0005 seconds.\n",
            "[2, 4, 18, 45, 45, 54, 60, 60, 85, 85, 87, 100, 105, 106, 107, 110, 119, 121, 124, 138, 141, 144, 144, 145, 145, 149, 161, 187, 192, 193]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How It Works**\n",
        "1. **Initialization:**\n",
        "\n",
        "    - Open all files and read the first line from each file to initialize the pointers.\n",
        "    - Store these first values in a pointers list.\n",
        "2. **Find the Smallest Value:**\n",
        "\n",
        "    - Use the built-in min() function to find the smallest value in the current pointers.\n",
        "3. **Update Pointers:**\n",
        "\n",
        "    - Determine which file contributed the smallest value and update its pointer by reading the next line from that file.\n",
        "4. **Repeat:**\n",
        "\n",
        "    - Continue until all files are fully read and no values remain in the pointers list.\n",
        "5. **Write Output:**\n",
        "\n",
        "    - Store the merged values in a list or write them directly to a file."
      ],
      "metadata": {
        "id": "BG3AzUvVgilB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 4: Word Counting\n",
        "Count word occurrences in a large text dataset using both naive (sequential) and partitioned (distributed) methods. This exercise simulates MapReduce-style word counting.\n"
      ],
      "metadata": {
        "id": "_E72pfzQefqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1: Dataset Preparation\n",
        "Prepare a large text dataset (text_data.txt). For simplicity, let's create a file with random sentences."
      ],
      "metadata": {
        "id": "80v7O3HaeqZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "# Generate random sentences\n",
        "def generate_text_file(filename, num_lines=1000000):\n",
        "    words = [\"apple\", \"banana\", \"orange\", \"grape\", \"pineapple\", \"kiwi\", \"melon\"]\n",
        "    with open(filename, \"w\") as file:\n",
        "        for _ in range(num_lines):\n",
        "            line = \" \".join(random.choices(words, k=random.randint(5, 15)))\n",
        "            file.write(f\"{line}\\n\")\n",
        "\n",
        "generate_text_file(f\"text_data.txt\")"
      ],
      "metadata": {
        "id": "Rx2POwt4euS3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2: Sequential Word Count\n",
        "Count word occurrences by scanning the file line by line."
      ],
      "metadata": {
        "id": "nkB60B0xe2p_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "@timed\n",
        "def sequential_word_count(filename):\n",
        "    word_counts = defaultdict(int)\n",
        "    with open(filename, \"r\") as file:\n",
        "        for line in file:\n",
        "            for word in line.strip().split():\n",
        "                word_counts[word] += 1\n",
        "    return word_counts\n",
        "\n",
        "# Example usage\n",
        "word_counts = sequential_word_count(\"text_data.txt\")\n",
        "word_counts = sorted(word_counts.items())\n",
        "print(word_counts[:10])  # Print top 10 word counts\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxGjvfyXe5JH",
        "outputId": "7f0c2516-83af-4b8e-e4db-b73be7acc90f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'sequential_word_count' executed in 2.6121 seconds.\n",
            "[('apple', 1429847), ('banana', 1425329), ('grape', 1427017), ('kiwi', 1431207), ('melon', 1428040), ('orange', 1428904), ('pineapple', 1426944)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 3: Partitioned Word Count\n",
        "Use a hash function to divide the dataset into smaller files, process each partition, and combine results.\n"
      ],
      "metadata": {
        "id": "GT5VaB5We8ai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Partition the Dataset**\n",
        "Partition the dataset into smaller files based on a hash function."
      ],
      "metadata": {
        "id": "y52H2WCMfESW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def partition_hash(key, num_partitions):\n",
        "    return sum(ord(c) for c in key) % num_partitions\n",
        "\n",
        "\n",
        "def partition_text_file(input_file, partitions=10):\n",
        "    partition_files = [open(f\"text_partition_{i}.txt\", \"w\") for i in range(partitions)]\n",
        "    with open(input_file, \"r\") as file:\n",
        "        for line in file:\n",
        "            words = line.strip().split()\n",
        "            for word in words:\n",
        "                partition_index = partition_hash(word, partitions)\n",
        "                partition_files[partition_index].write(f\"{word}\\n\")\n",
        "    for pf in partition_files:\n",
        "        pf.close()\n",
        "\n",
        "\n",
        "partition_text_file(\"text_data.txt\")\n"
      ],
      "metadata": {
        "id": "EM9wI-mGfHGu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Count Words in Each Partition**\n",
        "Count word occurrences in each partition file."
      ],
      "metadata": {
        "id": "GAJQke8hfJSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_partition_file(filename):\n",
        "    word_counts = defaultdict(int)\n",
        "    with open(filename, \"r\") as file:\n",
        "        for line in file:\n",
        "            word_counts[line.strip()] += 1\n",
        "    return word_counts"
      ],
      "metadata": {
        "id": "GdNOBBlDfNmr"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combine Results**\n",
        "Aggregate word counts from all partitions."
      ],
      "metadata": {
        "id": "DtlkFzk7fPOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@timed\n",
        "def partitioned_word_count(partitions=10):\n",
        "    partition_counts = []\n",
        "    # Compute all local word count\n",
        "    for i in range(partitions):  # can be parallized\n",
        "        local_count = count_partition_file(f\"text_partition_{i}.txt\")\n",
        "        partition_counts.append(local_count)\n",
        "\n",
        "    # Aggregate the local word count\n",
        "    combined_counts = defaultdict(int)\n",
        "    for partition in partition_counts:\n",
        "        for word, count in partition.items():\n",
        "            combined_counts[word] += count\n",
        "\n",
        "    return combined_counts\n",
        "\n",
        "# Example usage\n",
        "partitioned_counts = partitioned_word_count(10)\n",
        "partitioned_counts = sorted(partitioned_counts.items())\n",
        "print(partitioned_counts[:10])  # Print top 10 word counts\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPVUFGNHfR88",
        "outputId": "fad4eb5d-551a-4719-97db-c7ececdb4bd8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'partitioned_word_count' executed in 2.7242 seconds.\n",
            "[('apple', 1429847), ('banana', 1425329), ('grape', 1427017), ('kiwi', 1431207), ('melon', 1428040), ('orange', 1428904), ('pineapple', 1426944)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Discussion\n",
        "**Sequential Approach:** Processes the entire dataset in one pass but can be slow for very large datasets due to memory constraints.  \n",
        "**Partitioned Approach:** Divides work across multiple files, simulating parallel processing and reducing memory usage. This approach is scalable and forms the basis of MapReduce-style word counting.\n"
      ],
      "metadata": {
        "id": "VDkoXrf9fTbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Key Takeaway\n",
        "The partitioned approach is more scalable for large datasets, demonstrating how the \"map\" and \"reduce\" steps in distributed frameworks like Hadoop and Spark optimize big data processing."
      ],
      "metadata": {
        "id": "7najgvy1fc5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Detailed Analysis**\n",
        "\n",
        "1. **Exercise 1: Searching**\n",
        "   - **Naive Approach**: Sequentially scans the entire file for each search query, which scales poorly as \\(n\\) (number of integers) grows.\n",
        "   - **Partitioned Approach**: Limits the search to a smaller subset of the data by hashing, improving performance as \\(m\\) (number of partitions) increases.\n",
        "\n",
        "2. **Exercise 2: Grouping**\n",
        "   - **Naive Approach**: Directly aggregates values for each key in a single pass.\n",
        "   - **Partitioned Approach**: Divides the dataset into \\(m\\) smaller groups, reducing memory overhead and simulating distributed parallel processing.\n",
        "\n",
        "3. **Exercise 3: Merge-Sort**\n",
        "   - **Pointer-Based Merge**: Simpler but less efficient, as each merge step compares all \\(k\\) current elements.\n",
        "   - **`heapq` Merge**: Maintains a min-heap to quickly find the smallest element, reducing comparison overhead.\n",
        "\n",
        "4. **Exercise 4: Word Count**\n",
        "   - **Naive Approach**: Reads the dataset sequentially and counts words in memory, which becomes slow for very large datasets.\n",
        "   - **Partitioned Approach**: Uses hashing to divide data into manageable chunks, allowing efficient in-memory counting for each partition.\n"
      ],
      "metadata": {
        "id": "ALx0I5FohmHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Takeaways**\n",
        "- **Partitioning**: Improves scalability in Exercises 1, 2, and 4 by reducing the size of the data each step processes.\n",
        "- **Parallelism**: Many \"advanced\" methods simulate distributed systems, which are inherently more scalable for big data problems.\n"
      ],
      "metadata": {
        "id": "3tIOd5Hhh9Jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Track"
      ],
      "metadata": {
        "id": "GZF4LXCUjfNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in function_perf_tracker.items():\n",
        "  print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np1A4qefjiHe",
        "outputId": "e1a83a92-8643-482c-f089-96c033f23745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear_search: 0.6784422397613525\n",
            "partitioned_search: 0.0029621124267578125\n",
            "naive_grouping: 28.416772603988647\n",
            "partitioned_grouping: 28.62277603149414\n",
            "iterator_based_grouping: 3.09828782081604\n",
            "grouping_by_iteration: 2.3079564571380615\n",
            "n_way_merge: 0.0011546611785888672\n",
            "sequential_word_count: 4.4674296379089355\n",
            "partitioned_word_count: 4.637244701385498\n"
          ]
        }
      ]
    }
  ]
}