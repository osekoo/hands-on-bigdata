{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab Session: Algorithms and Programming with Spark RDDs using PySpark**  \n",
        "\n",
        "## Introduction\n",
        "This lab session introduces you to foundational concepts of distributed data processing using Spark's Resilient Distributed Datasets (RDDs). You'll leverage Python and the PySpark library within the Colab environment to build, execute, and analyze various Spark programs. The session covers:\n",
        "\n",
        "- **Setting up PySpark in Colab**: Learn to configure and initialize the SparkContext.\n",
        "- **Practical Exercises**:\n",
        "  1. **Word Count Problem**: Implement a Spark algorithm to analyze word frequencies in a text file.\n",
        "  2. **Data Aggregation**: Compute the average quantities from a sample dataset while minimizing shuffle operations.\n",
        "  3. **Join Operations**: Explore algorithms to perform equi-joins and right-outer joins on RDDs without the direct `join()` transformation.\n",
        "  4. **SQL Query Encoding in Spark**: Encode and test SQL-like queries using Python MapReduce transformations.\n",
        "\n",
        "Each exercise is designed to deepen your understanding of Spark's capabilities, RDD transformations, and actions. You'll also develop skills in optimizing performance and implementing complex operations.\n",
        "\n",
        "**Tools Required**:\n",
        "- **Python**: For Spark programming.\n",
        "- **Colab Notebook**: To run your Spark scripts.\n",
        "\n",
        "By the end of this session, you'll have hands-on experience in using Spark for solving real-world problems effectively."
      ],
      "metadata": {
        "id": "-BsbMQnibpts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 1: Word Count Problem**\n",
        "**Objective**: Design and implement a Spark algorithm to compute word frequencies in a text file.\n",
        "\n",
        "1. **Setup**:  \n",
        "   - Ensure your Colab environment is ready with Spark installed. Run the following commands:"
      ],
      "metadata": {
        "id": "agGOOOPmb4Nw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zXI3dk_bWXv"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark\n",
        "from pyspark import SparkConf, SparkContext\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Download or use an existing text file (e.g., `shake.txt`)."
      ],
      "metadata": {
        "id": "plWVlt2RcLrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/s/7ae58iydjloajvt/shake.txt"
      ],
      "metadata": {
        "id": "1viu72hmcMhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Create the RDD**:\n",
        "   - Load the text file into an RDD:"
      ],
      "metadata": {
        "id": "wGZ7nJ7acSQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document = sc.textFile(\"shake.txt\")"
      ],
      "metadata": {
        "id": "tBUyA68McUWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Transform and Process**:\n",
        "   - Tokenize the lines into words:"
      ],
      "metadata": {
        "id": "Bade34VEcXDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = None"
      ],
      "metadata": {
        "id": "2Io21o3Ccex5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Map each word to a key-value pair:"
      ],
      "metadata": {
        "id": "jIJJ0Oleci0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_pairs = None"
      ],
      "metadata": {
        "id": "TIaNeXZUclwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Reduce by key to count occurrences:"
      ],
      "metadata": {
        "id": "4-8ave5ocozd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts = None"
      ],
      "metadata": {
        "id": "XZ_0gZeTcqY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **View Results**:\n",
        "   - Display the word counts:"
      ],
      "metadata": {
        "id": "ckMgpVbJcwGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_counts.collect() if word_counts else 'Not computed yet')"
      ],
      "metadata": {
        "id": "5XFvng7ocwkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 2: Data Aggregation**\n",
        "**Objective**: Compute the average quantity of each pet from a dataset and analyze shuffle operations.\n",
        "\n",
        "1. **Setup**:  \n",
        "   - Create an RDD for the dataset. For example:"
      ],
      "metadata": {
        "id": "VB1qsLbrc34e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"dog\", 3), (\"cat\", 4), (\"dog\", 5), (\"cat\", 6)]\n",
        "pets = sc.parallelize(data)"
      ],
      "metadata": {
        "id": "l5iZiOJQc58g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Aggregate Data**:\n",
        "   - Calculate the total quantity and count for each pet:"
      ],
      "metadata": {
        "id": "iGEMzba7c9pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "totals = None"
      ],
      "metadata": {
        "id": "1h1wKI_5c_8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Compute the average:"
      ],
      "metadata": {
        "id": "tPJo07HbdCvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "averages = None"
      ],
      "metadata": {
        "id": "EtxwC_vgdFBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. **Optimize Shuffle**:\n",
        "   - How to reduce shuffle operations?\n",
        "\n",
        "4. **View Results**:\n",
        "   - Print the averages:"
      ],
      "metadata": {
        "id": "86CIfKZ3dHsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(averages.collect() if averages else 'Not computed yet')"
      ],
      "metadata": {
        "id": "OqOx4DFKdRmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Exercise 3: Join Operations**\n",
        "**Objective**: Perform equi-joins and right-outer joins without the `join()` transformation.\n",
        "\n",
        "1. **Setup**:  \n",
        "   - Use two RDDs representing key-value datasets:"
      ],
      "metadata": {
        "id": "M-L1Bp9GdWgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1 = sc.parallelize([(\"A\", 1), (\"B\", 2), (\"C\", 3)])\n",
        "rdd2 = sc.parallelize([(\"A\", 4), (\"B\", 5), (\"D\", 6)])"
      ],
      "metadata": {
        "id": "Vu678v4idaGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Equi-Join Implementation**:\n",
        "   - Perform a cartesian product and filter:"
      ],
      "metadata": {
        "id": "o_BfijZudc4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "equi_join = None"
      ],
      "metadata": {
        "id": "YKrseRrfdfK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Right-Outer Join**:\n",
        "   - Extend the equi-join logic to include keys exclusive to `rdd2`.\n",
        "\n",
        "4. **Discuss Results**:\n",
        "   - Compare performance with the standard `join()` transformation.\n"
      ],
      "metadata": {
        "id": "BGls7mtEdjng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 4: Encoding SQL Queries in Spark**\n",
        "**Objective**: Encode SQL-like queries using Python MapReduce and test them.\n",
        "\n",
        "1. **Setup**:  \n",
        "   - Download the files:"
      ],
      "metadata": {
        "id": "gcooq_iTdpAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/s/tmt6u80mkrwfjkv/Customer.txt\n",
        "!wget https://www.dropbox.com/s/8n5cbmufqhzs4r3/Order.txt"
      ],
      "metadata": {
        "id": "zv83qgFXdnhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Load data into RDDs:"
      ],
      "metadata": {
        "id": "3qRNEVfpdurL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_rdd = sc.textFile(\"Customer.txt\").map(lambda line: line.split(\",\"))\n",
        "order_rdd = sc.textFile(\"Order.txt\").map(lambda line: line.split(\",\"))"
      ],
      "metadata": {
        "id": "u0aIkwu-dwq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Query 1: Customers with Orders in July**:\n",
        "   - Filter customers by month:   \n",
        "   ``SELECT name FROM Customer WHERE month(startDate)=7``"
      ],
      "metadata": {
        "id": "B11KQDN4dzTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "july_customers = None\n",
        "print(july_customers.collect() if july_customers else 'Not computed yet')"
      ],
      "metadata": {
        "id": "xsAxQtkYd1YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Query 2: Distinct Names**:\n",
        "   - Use `distinct()`:  \n",
        "   `SELECT DISTINCT name FROM Customer WHERE month(startDate)=7`"
      ],
      "metadata": {
        "id": "HBowT5poeOzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distinct_names = None\n",
        "print(distinct_names.collect() if distinct_names else 'Not computed yet')"
      ],
      "metadata": {
        "id": "5ZguKPkQeSZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Query 3: Aggregated Orders**:\n",
        "   - Perform grouping and aggregation:  \n",
        "`SELECT  O.cid, SUM(total), COUNT(DISTINCT total)  FROM Order O GROUP BY O.cid`"
      ],
      "metadata": {
        "id": "KWx1OdxUeWxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_orders = None\n",
        "aggregated = None\n",
        "print(aggregated.collect() if aggregated else 'Not computed yet')"
      ],
      "metadata": {
        "id": "zxCWGe02eYn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Query 4: Join Customers and Orders**:\n",
        "   - Use a key-based join:  \n",
        "   `•\tSELECT C.cid, O.total FROM Customer C, Order O WHERE  C.cid=O.ci`"
      ],
      "metadata": {
        "id": "Gez-vAl4ecip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "join_result = None\n",
        "print(join_result.collect() if join_result else 'Not computed yet')"
      ],
      "metadata": {
        "id": "4lbryxkvedNQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}